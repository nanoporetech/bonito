#!/usr/bin/env python

"""
Convert a chunkify training file
"""

import os
import h5py
import random
import numpy as np
from bisect import bisect_left
from argparse import ArgumentParser
from itertools import chain, zip_longest


def align(samples, pointers):
    """ align to the start of the mapping """
    return samples[pointers[0]:], pointers - pointers[0]


def scale(read, samples, normalise=True):
    """ scale and normalise a read """
    scaling = read.attrs['range'] / read.attrs['digitisation']
    scaled = (scaling * (samples + read.attrs['offset'])).astype(np.float32)
    if normalise:
        return (scaled - read.attrs['shift_frompA']) / read.attrs['scale_frompA']
    return scaled


def num_reads(tfile):
    """ return the sample lengths for each read in the training file """
    with h5py.File(tfile, 'r') as training_file:
        return len(training_file['Reads'])


def get_reads(tfile):
    """ get each dataset per read """
    with h5py.File(tfile, 'r') as training_file:
        for read_id in np.random.permutation(training_file['Reads']):
            read = training_file['Reads/%s' % read_id]
            reference = read['Reference'][:]
            pointers = read['Ref_to_signal'][:]
            samples = read['Dacs'][:]
            samples = scale(read, samples)
            samples, pointers = align(samples, pointers)
            yield read_id, samples, reference, pointers


def main(args):

    random.seed(args.seed)
    np.random.seed(args.seed)
    os.makedirs(args.output_directory, exist_ok=True)

    read_idx = 0
    chunk_idx = 0
    chunk_count = 0

    min_bases = 0
    max_bases = 0
    off_the_end_ref = 0
    off_the_end_sig = 0
    min_samples_per_base = 8
    max_samples_per_base = 16

    total_reads = num_reads(args.chunkify_file)

    chunks = np.zeros((args.chunks, args.max_seq_len * max_samples_per_base), dtype=np.float32)
    chunk_lengths = np.zeros(args.chunks, dtype=np.uint16)

    targets = np.zeros((args.chunks, args.max_seq_len), dtype=np.uint8)
    target_lengths = np.zeros(args.chunks, dtype=np.uint8)

    for read_id, samples, reference, pointers in get_reads(args.chunkify_file):

        read_idx += 1
        num_samples = np.int32(len(reference) * args.samples_per_read)

        sequence_length = len(reference)
        squiggle_duration = len(samples)

        seq_starts = np.random.randint(0, sequence_length, size=num_samples)
        seq_ends   = np.random.randint(args.min_seq_len, args.max_seq_len, size=num_samples) + seq_starts

        for start, end in zip(seq_starts, seq_ends):

            chunk_idx += 1

            if end > sequence_length:
                off_the_end_ref += 1
                continue

            squiggle_start = pointers[start]
            squiggle_end = pointers[end]
            squiggle_length = squiggle_end - squiggle_start

            reference_length = end - start

            samples_per_base = squiggle_length / reference_length

            if samples_per_base < min_samples_per_base:
                min_bases += 1
                continue

            if samples_per_base > max_samples_per_base:
                max_bases += 1
                continue

            if squiggle_end > squiggle_duration:
                off_the_end_sig += 1
                continue

            chunks[chunk_count, :squiggle_length] = samples[squiggle_start:squiggle_end]
            chunk_lengths[chunk_count] = squiggle_length

            # index alphabet from 1 (ctc blank labels - 0)
            targets[chunk_count, :reference_length] = reference[start:end] + 1
            target_lengths[chunk_count] = reference_length

            chunk_count += 1

            if chunk_count == args.chunks:
                break

        if chunk_count == args.chunks:
            break

    skipped = chunk_idx - chunk_count
    percent = (skipped / chunk_count * 100) if skipped else 0

    print("Processed %s reads of out %s [%.2f%%]" % (read_idx, total_reads, read_idx / total_reads * 100))
    print("Skipped %s chunks out of %s due to bad chunks [%.2f%%].\n" % (skipped, chunk_count, percent))
    print("Reason for skipping:")
    print("  - off the end (signal)          ", off_the_end_sig)
    print("  - off the end (sequence)        ", off_the_end_ref)
    print("  - minimum number of bases       ", min_bases)
    print("  - maximum number of bases       ", max_bases)

    if chunk_count < args.chunks:
        chunks = np.delete(chunks, np.s_[chunk_count:], axis=0)
        chunk_lengths = chunk_lengths[:chunk_count]
        targets = np.delete(targets, np.s_[chunk_count:], axis=0)
        target_lengths = target_lengths[:chunk_count]

    np.save(os.path.join(args.output_directory, "chunks.npy"), chunks)
    np.save(os.path.join(args.output_directory, "chunk_lengths.npy"), chunk_lengths)
    np.save(os.path.join(args.output_directory, "references.npy"), targets)
    np.save(os.path.join(args.output_directory, "reference_lengths.npy"), target_lengths)

    print()
    print("Training data written to %s:" % args.output_directory)
    print("  - chunks.npy with shape", chunks.shape)
    print("  - chunk_lengths.npy with shape", chunk_lengths.shape)
    print("  - references.npy with shape", targets.shape)
    print("  - reference_lengths.npy shape", target_lengths.shape)


if __name__ == "__main__":
    parser = ArgumentParser()
    parser.add_argument("chunkify_file")
    parser.add_argument("output_directory")
    parser.add_argument("--seed", default=25, type=int)
    parser.add_argument("--chunks", default=1000, type=int)
    parser.add_argument("--min-seq-len", default=128, type=int)
    parser.add_argument("--max-seq-len", default=256, type=int)
    parser.add_argument("--samples-per-read", default=0.03, type=float)
    main(parser.parse_args())
